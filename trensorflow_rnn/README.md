# RNN介绍



经典的人工神经网络、深度神经网络（DNN），甚至卷积神经网络（CNN），一是输入的数据维度相同，另外是各个输入之间是独立的，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立。

![nn-pic](imgs_md/nn.png)

在实际应用过程中，例如对一个演讲进行语音识别，那演讲者每讲一句话的时间几乎都不太相同，而识别演讲者的讲话内容必须要按照讲话的顺序进行识别；再例如对一段文字信息进行分析，按照文字前后顺序进行内容填空，以上两个例子中DNN和CNN都无法有效的解决问题。这就需要有一种能力更强的模型：**该模型具有一定的记忆能力**，能够按时序依次处理任意长度的信息，这个模型就是“循环神经网络”（Recurrent Neural Networks，简称RNN）。

循环神经网络（RNN）的神经元（rnn_cell）的输出可以在下一个时间戳直接作用到自身（作为输入），看看下面的对比图：

![nn-vs-rnn](imgs_md/nn_vs_rnn.png)

从上面的两个简化图，可以看出RNN相比经典的神经网络结构多了一个循环圈，这个圈就代表着神经元的输出在下一个时间戳还会返回来作为输入的一部分，这些循环让RNN看起来似乎很神秘，然而，换个角度想想，也不比一个经典的神经网络难于理解。RNN可以被看做是对同一神经网络的多次赋值，第i层神经元在t时刻的输入，除了（i-1）层神经元在该时刻的输出外，还包括其自身在（t-1）时刻的输出，如果我们按时间点将RNN展开，将得到以下的结构图：

![rnn-pic](imgs_md/rnn.png)

在不同的时间点，RNN的输入都与将之前的时间状态有关，tn时刻网络的输出结果是该时刻的输入和所有历史共同作用的结果，这就达到了**对时间序列建模**的目的。